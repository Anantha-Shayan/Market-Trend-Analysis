# -*- coding: utf-8 -*-
"""Time-Series Forecasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dlXHj-Ji9vaMUj3M4Zo8bVn91bSf7KGg
"""

# import pandas as pd
# import glob
# import os

# # 1. Get a list of all CSV file paths
# path = '/content/drive/MyDrive/Agmarket/Price'
# all_files = glob.glob(os.path.join(path, "*.csv"))

# # 2. Use a list comprehension to read all files
# # We add a column 'Commodity' based on the filename for tracking
# df_list = []
# for filename in all_files:
#     df = pd.read_csv(filename)
#     # Extract filename without extension as the label
#     df['commodity_name'] = os.path.basename(filename).replace('.csv', '')
#     df_list.append(df)

# # 3. Combine everything into one DataFrame
# combined_df = pd.concat(df_list, axis=0, ignore_index=True)

# # 4. Export to a high-performance format
# combined_df.to_csv('combined_agriculture_data.csv', index=False)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Data Collection**"""

data = pd.read_csv('/content/drive/MyDrive/Agmarket/Price/combined_agriculture_data.csv')

df = pd.DataFrame(data)
df

df = df.drop(['commodity_name','market_id','state_id','district_id'], axis=1)

df.rename(columns={'t': 'Date','cmdty':'Commodity','market_name':'Market','state_name':'State','district_name':'District','variety':'Variety'}, inplace=True)

df = df.sort_values(by=['Date', 'Commodity', 'State'],ignore_index=True)

"""# **EDA and pre-processing**"""

# df.info()
# df.set_index('Date', inplace=True)

print(df['Commodity'].nunique())
print(df['Market'].nunique())
print(df['State'].nunique())

# outliers
invalid_prices = df[
    (df['p_min'] > df['p_modal']) | (df['p_modal'] > df['p_max'])
]

len(invalid_prices)

# Check if price is -ve (outliers again)
(df[['p_min', 'p_max', 'p_modal']] < 0).sum()

df['p_modal'].describe()

# Check if data is balanced
df['Commodity'].value_counts().head(10)

df

df['Commodity'].unique()

# check unique for state, district, market, variety
df['State'].unique()

df['District'].nunique()

df.isna().sum()

#df = df.reset_index()

df

def missing_dates(group):
    full_range = pd.date_range(group['Date'].min(), group['Date'].max())
    return len(full_range) - group['Date'].nunique()

missing_by_group = (
    df.groupby(['Commodity', 'Market'])
      .apply(missing_dates)
      .sort_values(ascending=False)
      .head(10)
)

missing_by_group

df['Date'] = pd.to_datetime(df['Date'])

df['year'] = df['Date'].dt.year
df['month'] = df['Date'].dt.month
df['week'] = df['Date'].dt.isocalendar().week
df['dayofweek'] = df['Date'].dt.dayofweek

potato = df[(df['Commodity'] == 'Tomato') & (df['year'] == 2025)]

monthly_avg = (
    potato.groupby('month')['p_modal']
    .mean()
)

monthly_avg

potato_market_stats = (
    potato.groupby('State')['p_modal']
    .mean()
    .sort_values(ascending=False)
    .head(10)
)

potato_market_stats

df = df.sort_values(['Commodity', 'Market', 'Date'], ignore_index = True)

df['price_change'] = (
    df.groupby(['Commodity', 'Market'])['p_modal']
      .diff()
)

df['pct_change'] = (
    df.groupby(['Commodity', 'Market'])['p_modal']
      .pct_change()
)

df

print(df['price_change'].isna().sum())
print(df['pct_change'].isna().sum())

df['price_spike'] = df['pct_change'] > 0.15  # 15% daily spike
df['price_spike'].mean()

final_df_for_analysis = df.copy('')
final_df_for_analysis

df['State_District_Market'] = df['State'] + '_' + df['District'] + '_' + df['Market']
df

"""# **Data Preparation**"""

deepar_df = df[['Date', 'Commodity', 'State_District_Market', 'p_modal']].copy()
deepar_df['Union'] = df['Commodity'] + "_" + df['State_District_Market']
deepar_df = deepar_df.sort_values(by=['Date', 'Union'], ignore_index=True)
deepar_df = deepar_df.drop('State_District_Market', axis = 1)
deepar_df

"""DeepAR requires an integer time index, not datetime.
Because internally
DeepAR uses RNNs and
RNNs operate on ordered sequences, not calendar dates
"""

deepar_df['time_idx'] = (
        deepar_df.groupby('Union')['Date']
          .rank(method='dense')
          .astype(int)
    )

deepar_df.drop(['Commodity'], axis=1, inplace=True)

deepar_df = deepar_df.dropna(subset=["p_modal"])

deepar_test = deepar_df[deepar_df["Date"] >= "2023-01-01"].copy()
deepar_train = deepar_df[deepar_df["Date"] < "2023-01-01"].copy() #same data for train and val

deepar_test.drop(['Date'], axis=1, inplace=True)
deepar_train.drop(['Date'], axis=1, inplace=True)

deepar_train

"""# **DeepAR**"""

!pip install --no-cache-dir -v lightning pytorch-forecasting

!pip uninstall pytorch-lightning

!pip show lightning
!pip show pytorch-forecasting

import pandas as pd
import torch
import lightning as pl
from pytorch_forecasting import DeepAR
from pytorch_forecasting.data import (TimeSeriesDataSet,GroupNormalizer)
from pytorch_forecasting.metrics import NormalDistributionLoss

# deepar_train = deepar_df[deepar_df['Date'] < '2022-01-01'].copy()
# deepar_val = deepar_df[(deepar_df['Date'] >= '2022-01-01') & (deepar_df['Date'] < '2023-01-01')].copy()
# deepar_test = deepar_df[deepar_df['Date'] >= '2023-01-01'].copy()

"""Why shorter encoder is BETTER for agri price data
> Reason 1 — Local dynamics dominate
Short-term price movement is driven by Recent supply, demand  
Not by
Prices from 2 years ago  
So the last 10–14 observations are far more predictive than the last 30–60.

> Reason 2 — Long histories introduce noise
"""

max_prediction_length = 7
max_encoder_length = 14

training_cut_off = deepar_train['time_idx'].max() - max_prediction_length

training = TimeSeriesDataSet(
    deepar_train[deepar_train.time_idx <= training_cut_off],
    time_idx="time_idx",
    target="p_modal",
    group_ids=["Union"],
    max_encoder_length=max_encoder_length,
    max_prediction_length=max_prediction_length,
    time_varying_unknown_reals=["p_modal"],
    target_normalizer=GroupNormalizer(groups=["Union"]), #normalize y
    allow_missing_timesteps=True
)

validation = TimeSeriesDataSet.from_dataset(
    training,  #inherit all imp parameters like 'groupby', 'max_enco','max_pred', etc., from training set
    deepar_train,
    predict=True, #this says that the set is for prediction (therefore validation set)
    stop_randomization=True
    #stop randomization of enc and deco len within specified min,max_enc_len and min,max_pred_len ranges cuz you want a consistent and reproducible benchmark.
    # this is false in training cuz it exposes the model to a wider variety of i/p sequence len and pred horizons during training, making it more robust and less prone to overfitting to specific sequence len.
)

# Data Loader
batch_size = 64
train_loader = training.to_dataloader(
    train=True, # Randomly samples forecast windows and shuffles them (shuffles windows not time order) which improves generalization
    batch_size=batch_size,
    num_workers=2
)

val_loader = validation.to_dataloader(
    train=False, # No randomization. No gradient updates
    batch_size=batch_size,
    num_workers=2
)

#model definition
model = DeepAR.from_dataset(
    training,
    learning_rate=1e-3, #DeepAR uses Adam optimizer. Adam is stable in the range: 1e-4 to 1e-3
    hidden_size=64,
    rnn_layers=2, # layer1 - short term, layer2 - mideium term
    # 3 is overkill, 1 leads to overfit
    dropout=0.1,
    loss=NormalDistributionLoss(),
    # DeepAR models a probability distribution, not quantiles directly — therefore it must be trained with a distribution-based loss.
)

# train
trainer = pl.Trainer(
    max_epochs=20,
    accelerator="auto",
    gradient_clip_val=0.1, # to prevent exploding gradients during training
    enable_model_summary=True
)

trainer.fit(
    model,
    train_dataloaders=train_loader,
    val_dataloaders=val_loader
)

# test_dataset = TimeSeriesDataSet.from_dataset(
#     training,
#     deepar_test,
#     predict=True,
#     stop_randomization=True
# )

!pip list | grep -E "lightning|forecasting"

import pytorch_forecasting
from pytorch_forecasting import DeepAR

import inspect

print(pytorch_forecasting.__version__)
print(inspect.getmodule(DeepAR))

import lightning.pytorch as pl
from pytorch_forecasting import DeepAR
from pytorch_forecasting.metrics import NormalDistributionLoss

model = DeepAR.from_dataset(
    training,
    learning_rate=1e-3,
    hidden_size=64,
    rnn_layers=2,
    dropout=0.1,
    loss=NormalDistributionLoss(),
)

print(type(model))
print(pl.LightningModule)
print(isinstance(model, pl.LightningModule))

