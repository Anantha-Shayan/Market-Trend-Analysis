{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ebbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Time-Series Forecasting.ipynb\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1dlXHj-Ji9vaMUj3M4Zo8bVn91bSf7KGg\n",
    "\"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# # 1. Get a list of all CSV file paths\n",
    "# path = '/content/drive/MyDrive/Agmarket/Price'\n",
    "# all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "# # 2. Use a list comprehension to read all files\n",
    "# # We add a column 'Commodity' based on the filename for tracking\n",
    "# df_list = []\n",
    "# for filename in all_files:\n",
    "#     df = pd.read_csv(filename)\n",
    "#     # Extract filename without extension as the label\n",
    "#     df['commodity_name'] = os.path.basename(filename).replace('.csv', '')\n",
    "#     df_list.append(df)\n",
    "\n",
    "# # 3. Combine everything into one DataFrame\n",
    "# combined_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "# # 4. Export to a high-performance format\n",
    "# combined_df.to_csv('combined_agriculture_data.csv', index=False)\n",
    "def prepare_data():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    data = pd.read_csv('./combined_agriculture_data.csv')\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df\n",
    "\n",
    "    df = df.drop(['commodity_name','market_id','state_id','district_id'], axis=1)\n",
    "\n",
    "    df.rename(columns={'t': 'Date','cmdty':'Commodity','market_name':'Market','state_name':'State','district_name':'District','variety':'Variety'}, inplace=True)\n",
    "\n",
    "    df = df.sort_values(by=['Date', 'Commodity', 'State'],ignore_index=True)\n",
    "\n",
    "    \"\"\"# **EDA and pre-processing**\"\"\"\n",
    "\n",
    "    # df.info()\n",
    "    # df.set_index('Date', inplace=True)\n",
    "\n",
    "    print(df['Commodity'].nunique())\n",
    "    print(df['Market'].nunique())\n",
    "    print(df['State'].nunique())\n",
    "\n",
    "    # outliers\n",
    "    invalid_prices = df[\n",
    "        (df['p_min'] > df['p_modal']) | (df['p_modal'] > df['p_max'])\n",
    "    ]\n",
    "\n",
    "    len(invalid_prices)\n",
    "\n",
    "    # Check if price is -ve (outliers again)\n",
    "    (df[['p_min', 'p_max', 'p_modal']] < 0).sum()\n",
    "\n",
    "    df['p_modal'].describe()\n",
    "\n",
    "    # Check if data is balanced\n",
    "    df['Commodity'].value_counts().head(10)\n",
    "\n",
    "    df\n",
    "\n",
    "    df['Commodity'].unique()\n",
    "\n",
    "    # check unique for state, district, market, variety\n",
    "    df['State'].unique()\n",
    "\n",
    "    df['District'].nunique()\n",
    "\n",
    "    df.isna().sum()\n",
    "\n",
    "    #df = df.reset_index()\n",
    "\n",
    "    df\n",
    "\n",
    "    def missing_dates(group):\n",
    "        full_range = pd.date_range(group['Date'].min(), group['Date'].max())\n",
    "        return len(full_range) - group['Date'].nunique()\n",
    "\n",
    "    missing_by_group = (\n",
    "        df.groupby(['Commodity', 'Market'])\n",
    "        .apply(missing_dates, include_groups=False)\n",
    "        .sort_values(ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    missing_by_group\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    df['year'] = df['Date'].dt.year\n",
    "    df['month'] = df['Date'].dt.month\n",
    "    df['week'] = df['Date'].dt.isocalendar().week\n",
    "    df['dayofweek'] = df['Date'].dt.dayofweek\n",
    "\n",
    "    potato = df[(df['Commodity'] == 'Tomato') & (df['year'] == 2025)]\n",
    "\n",
    "    monthly_avg = (\n",
    "        potato.groupby('month')['p_modal']\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    monthly_avg\n",
    "\n",
    "    potato_market_stats = (\n",
    "        potato.groupby('State')['p_modal']\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "\n",
    "    potato_market_stats\n",
    "\n",
    "    df = df.sort_values(['Commodity', 'Market', 'Date'], ignore_index = True)\n",
    "\n",
    "    df['price_change'] = (\n",
    "        df.groupby(['Commodity', 'Market'])['p_modal']\n",
    "        .diff()\n",
    "    )\n",
    "\n",
    "    df['pct_change'] = (\n",
    "        df.groupby(['Commodity', 'Market'])['p_modal']\n",
    "        .pct_change(fill_method=None)\n",
    "    )\n",
    "\n",
    "    df\n",
    "\n",
    "    print(df['price_change'].isna().sum())\n",
    "    print(df['pct_change'].isna().sum())\n",
    "\n",
    "    df['price_spike'] = df['pct_change'] > 0.15  # 15% daily spike\n",
    "    df['price_spike'].mean()\n",
    "\n",
    "    final_df_for_analysis = df.copy('')\n",
    "    final_df_for_analysis\n",
    "\n",
    "    df['State_District_Market'] = df['State'] + '_' + df['District'] + '_' + df['Market']\n",
    "    df\n",
    "\n",
    "    \"\"\"# **Data Preparation**\"\"\"\n",
    "\n",
    "    deepar_df = df[['Date', 'Commodity', 'State_District_Market', 'p_modal']].copy()\n",
    "    deepar_df['Union'] = df['Commodity'] + \"_\" + df['State_District_Market']\n",
    "    deepar_df = deepar_df.sort_values(by=['Date', 'Union'], ignore_index=True)\n",
    "    deepar_df = deepar_df.drop('State_District_Market', axis = 1)\n",
    "    deepar_df\n",
    "\n",
    "    \"\"\"DeepAR requires an integer time index, not datetime.\n",
    "    Because internally\n",
    "    DeepAR uses RNNs and\n",
    "    RNNs operate on ordered sequences, not calendar dates\n",
    "    \"\"\"\n",
    "\n",
    "    deepar_df['time_idx'] = (\n",
    "            deepar_df.groupby('Union')['Date']\n",
    "            .rank(method='dense')\n",
    "            .astype(int)\n",
    "        )\n",
    "\n",
    "    deepar_df.drop(['Commodity'], axis=1, inplace=True)\n",
    "\n",
    "    deepar_df = deepar_df.dropna(subset=[\"p_modal\"])\n",
    "\n",
    "    deepar_test = deepar_df[deepar_df[\"Date\"] >= \"2023-01-01\"].copy()\n",
    "    deepar_train = deepar_df[deepar_df[\"Date\"] < \"2023-01-01\"].copy() #same data for train and val\n",
    "\n",
    "    deepar_test.drop(['Date'], axis=1, inplace=True)\n",
    "    deepar_train.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "    deepar_train\n",
    "\n",
    "    return deepar_train, deepar_test\n",
    "\n",
    "\n",
    "def build_datasets(deepar_train, max_encoder_length, max_prediction_length):\n",
    "    from pytorch_forecasting.data import TimeSeriesDataSet, GroupNormalizer\n",
    "\n",
    "    training_cut_off = deepar_train['time_idx'].max() - max_prediction_length\n",
    "\n",
    "    training = TimeSeriesDataSet(\n",
    "        deepar_train[deepar_train.time_idx <= training_cut_off],\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"p_modal\",\n",
    "        group_ids=[\"Union\"],\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        time_varying_unknown_reals=[\"p_modal\"],\n",
    "        target_normalizer=GroupNormalizer(groups=[\"Union\"]), #normalize y\n",
    "        allow_missing_timesteps=True\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(\n",
    "        training,  #inherit all imp parameters like 'groupby', 'max_enco','max_pred', etc., from training set\n",
    "        deepar_train,\n",
    "        predict=True, #this says that the set is for prediction (therefore validation set)\n",
    "        stop_randomization=True\n",
    "        #stop randomization of enc and deco len within specified min,max_enc_len and min,max_pred_len ranges cuz you want a consistent and reproducible benchmark.\n",
    "        # this is false in training cuz it exposes the model to a wider variety of i/p sequence len and pred horizons during training, making it more robust and less prone to overfitting to specific sequence len.\n",
    "    )\n",
    "\n",
    "    return training, validation\n",
    "\n",
    "\n",
    "def build_model(training):\n",
    "    from pytorch_forecasting import DeepAR\n",
    "    from pytorch_forecasting.metrics import NormalDistributionLoss\n",
    "\n",
    "    model = DeepAR.from_dataset(\n",
    "        training,\n",
    "        learning_rate=1e-3, #DeepAR uses Adam optimizer. Adam is stable in the range: 1e-4 to 1e-3\n",
    "        hidden_size=64,\n",
    "        rnn_layers=2, # layer1 - short term, layer2 - mideium term\n",
    "        # 3 is overkill, 1 leads to overfit\n",
    "        dropout=0.1,\n",
    "        loss=NormalDistributionLoss(),\n",
    "        # DeepAR models a probability distribution, not quantiles directly â€” therefore it must be trained with a distribution-based loss.\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_trainer():\n",
    "    import lightning as pl\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=5,\n",
    "        accelerator=\"cuda\",\n",
    "        devices=1,\n",
    "        gradient_clip_val=0.1, # to prevent exploding gradients during training\n",
    "        enable_model_summary=True\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def train(checkpoint_path):\n",
    "    import os\n",
    "\n",
    "    deepar_train, _ = prepare_data()\n",
    "\n",
    "    max_prediction_length = 7\n",
    "    max_encoder_length = 14\n",
    "\n",
    "    training, validation = build_datasets(\n",
    "        deepar_train,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=max_prediction_length\n",
    "    )\n",
    "\n",
    "    batch_size = 64\n",
    "    train_loader = training.to_dataloader(\n",
    "        train=True, # Randomly samples forecast windows and shuffles them (shuffles windows not time order) which improves generalization\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    val_loader = validation.to_dataloader(\n",
    "        train=False, # No randomization. No gradient updates\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    model = build_model(training)\n",
    "    trainer = build_trainer()\n",
    "\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader\n",
    "    )\n",
    "    trainer.save_checkpoint(checkpoint_path)\n",
    "\n",
    "\n",
    "def evaluate_validation(checkpoint_path):\n",
    "    import torch\n",
    "\n",
    "    deepar_train, _ = prepare_data()\n",
    "\n",
    "    max_prediction_length = 7\n",
    "    max_encoder_length = 14\n",
    "\n",
    "    training, validation = build_datasets(\n",
    "        deepar_train,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=max_prediction_length\n",
    "    )\n",
    "\n",
    "    batch_size = 64\n",
    "    val_loader = validation.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    from pytorch_forecasting import DeepAR\n",
    "    model = DeepAR.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "\n",
    "    val_preds, val_x = model.predict(\n",
    "        val_loader,\n",
    "        mode=\"quantiles\",\n",
    "        quantiles=[0.1, 0.5, 0.9],\n",
    "        return_x=True\n",
    "    )\n",
    "\n",
    "    val_actuals = torch.cat([y[0] for y in val_x[\"decoder_target\"]])\n",
    "    val_median = val_preds[..., 1]\n",
    "    valid_mask = val_actuals != 0\n",
    "\n",
    "    val_mape = torch.mean(\n",
    "        torch.abs((val_actuals[valid_mask] - val_median[valid_mask]) / val_actuals[valid_mask])\n",
    "    )\n",
    "    val_mae = torch.mean(torch.abs(val_actuals - val_median))\n",
    "\n",
    "    print(\"VAL MAPE:\", val_mape.item())\n",
    "    print(\"VAL MAE :\", val_mae.item())\n",
    "\n",
    "\n",
    "def evaluate_test(checkpoint_path):\n",
    "    import torch\n",
    "    from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "\n",
    "    deepar_train, deepar_test = prepare_data()\n",
    "\n",
    "    max_prediction_length = 7\n",
    "    max_encoder_length = 14\n",
    "\n",
    "    training, _ = build_datasets(\n",
    "        deepar_train,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=max_prediction_length\n",
    "    )\n",
    "\n",
    "    test_dataset = TimeSeriesDataSet.from_dataset(\n",
    "        training,\n",
    "        deepar_test,\n",
    "        predict=True,\n",
    "        stop_randomization=True\n",
    "    )\n",
    "\n",
    "    batch_size = 64\n",
    "    test_loader = test_dataset.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    from pytorch_forecasting import DeepAR\n",
    "    model = DeepAR.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "\n",
    "    test_preds, test_x = model.predict(\n",
    "        test_loader,\n",
    "        mode=\"quantiles\",\n",
    "        quantiles=[0.1, 0.5, 0.9],\n",
    "        return_x=True\n",
    "    )\n",
    "\n",
    "    test_actuals = torch.cat([y[0] for y in test_x[\"decoder_target\"]])\n",
    "    test_median = test_preds[..., 1]\n",
    "    valid_mask = test_actuals != 0\n",
    "\n",
    "    test_mape = torch.mean(\n",
    "        torch.abs((test_actuals[valid_mask] - test_median[valid_mask]) / test_actuals[valid_mask])\n",
    "    )\n",
    "    test_mae = torch.mean(torch.abs(test_actuals - test_median))\n",
    "\n",
    "    print(\"TEST MAPE:\", test_mape.item())\n",
    "    print(\"TEST MAE :\", test_mae.item())\n",
    "\n",
    "\n",
    "def predict_and_signal(checkpoint_path):\n",
    "    import torch\n",
    "    from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "\n",
    "    deepar_train, deepar_test = prepare_data()\n",
    "\n",
    "    max_prediction_length = 7\n",
    "    max_encoder_length = 14\n",
    "\n",
    "    training, _ = build_datasets(\n",
    "        deepar_train,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=max_prediction_length\n",
    "    )\n",
    "\n",
    "    test_dataset = TimeSeriesDataSet.from_dataset(\n",
    "        training,\n",
    "        deepar_test,\n",
    "        predict=True,\n",
    "        stop_randomization=True\n",
    "    )\n",
    "\n",
    "    batch_size = 64\n",
    "    test_loader = test_dataset.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    from pytorch_forecasting import DeepAR\n",
    "    model = DeepAR.load_from_checkpoint(checkpoint_path)\n",
    "    model.eval()\n",
    "\n",
    "    test_preds, test_x = model.predict(\n",
    "        test_loader,\n",
    "        mode=\"quantiles\",\n",
    "        quantiles=[0.1, 0.5, 0.9],\n",
    "        return_x=True\n",
    "    )\n",
    "\n",
    "    i = 0\n",
    "    actual = test_x[\"decoder_target\"][i][0].cpu()\n",
    "    q10 = test_preds[i, :, 0].cpu()\n",
    "    q50 = test_preds[i, :, 1].cpu()\n",
    "    q90 = test_preds[i, :, 2].cpu()\n",
    "\n",
    "    last_price = test_x[\"encoder_target\"][i][-1].item()\n",
    "    mean_future = torch.mean(q50).item()\n",
    "    pct_change = (mean_future - last_price) / last_price if last_price != 0 else 0.0\n",
    "\n",
    "    if pct_change > 0.08:\n",
    "        signal = \"SELL_LATER\"\n",
    "    elif pct_change < -0.08:\n",
    "        signal = \"SELL_NOW\"\n",
    "    else:\n",
    "        signal = \"HOLD\"\n",
    "\n",
    "    print(\"Signal:\", signal)\n",
    "    print(\"Expected % change:\", pct_change * 100)\n",
    "\n",
    "\n",
    "def main(stage):\n",
    "    import os\n",
    "    import pytorch_forecasting\n",
    "    import inspect\n",
    "    from pytorch_forecasting import DeepAR\n",
    "    import lightning.pytorch as pl\n",
    "\n",
    "    checkpoint_path = \"deepar_agmarket.ckpt\"\n",
    "\n",
    "    if stage == \"train\":\n",
    "        train(checkpoint_path)\n",
    "    elif stage == \"val\":\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(\"Checkpoint not found. Run: python time_series_forecasting.py train\")\n",
    "        evaluate_validation(checkpoint_path)\n",
    "    elif stage == \"test\":\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(\"Checkpoint not found. Run: python time_series_forecasting.py train\")\n",
    "        evaluate_test(checkpoint_path)\n",
    "    elif stage == \"predict\":\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(\"Checkpoint not found. Run: python time_series_forecasting.py train\")\n",
    "        predict_and_signal(checkpoint_path)\n",
    "    else:\n",
    "        raise ValueError(\"Stage must be one of: train | val | test | predict\")\n",
    "\n",
    "    print(pytorch_forecasting.__version__)\n",
    "    print(inspect.getmodule(DeepAR))\n",
    "    print(pl.LightningModule)\n",
    "\n",
    "    # !pip install --no-cache-dir -v lightning pytorch-forecasting\n",
    "\n",
    "    # !pip uninstall pytorch-lightning\n",
    "\n",
    "    # !pip show lightning\n",
    "    # !pip show pytorch-forecasting\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    import torch.multiprocessing as mp\n",
    "\n",
    "    mp.freeze_support()\n",
    "\n",
    "    if len(sys.argv) < 2:\n",
    "        raise ValueError(\"Usage: python time_series_forecasting.py [train|val|test|predict]\")\n",
    "\n",
    "    main(sys.argv[1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
